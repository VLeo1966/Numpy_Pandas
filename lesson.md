Не меняя текста отформатируй текст в makedown формате:

Теория на сегодня:

1. Этапы анализа данных и сферы его применения


Мы приступаем к анализу данных и сегодня будем говорить об обработке данных с помощью Pandas.

План урока таков:

Мы узнаем, что такое анализ данных и для чего он нужен;
Разберём основы использования библиотеки Pandas.
В прошлом уроке мы научились парсить информацию с сайтов, которую потом можно собрать в базы данных и таблицы для дальнейшего анализа.

✅Анализ данных в Python — это процесс извлечения полезной информации из данных. Этот процесс включает в себя сбор, обработку, визуализацию и интерпретацию данных.

Поговорим о каждом из этапов анализа данных более подробно.

Сбор данных. В качестве примера на прошлом уроке мы собрали четыре столбца данных. Конечно, статистические данные обычно занимают гораздо больший объём. Например, при сборе информации о фильмах потребуется собрать количество отзывов, оценки на различных платформах, жанры, названия, актёрский состав и прочую информацию, которая может быть полезна для анализа. При этом важно понимать, какая именно информация нужна, поскольку не все данные могут быть полезными.

Существует множество готовых наборов данных, которые можно использовать для обучения. Как правило, заказывая анализ данных, компания уже предоставляет аналитику все имеющиеся данные для этой задачи.

Очистка данных. Данные часто содержат ошибки, так как собираются в больших объёмах. Они могут включать пропущенные значения, если при парсинге информации с сайта что-то пошло не так. Некоторые собранные сведения могут быть ошибочными; также данные могут быть задублированы, что может исказить дальнейший анализ. Процесс очистки данных заключается в устранении всех ошибок, заполнении пропущенных данных и удалении дубликатов.

Обработка данных. На этом этапе данные преобразуются в формат, удобный для анализа, что мы уже обсуждали на прошлом уроке. Изначально данные могут быть в текстовом формате, но для анализа нам нужны числа, чтобы можно было их сравнивать, например, рейтинги, заработные платы и т.д.

Моделирование данных. Это своего рода прогнозирование и классификация данных. Поняв и проанализировав текущие тренды, можно определить, например, какой контент стоит выкладывать в социальных сетях, чтобы привлечь внимание аудитории. Аналогично, моделирование применяют по отношению к зарплатам, стоимости квартир и т.д.

Визуализация данных. Этот процесс нужен для упрощения восприятия анализа. На этом этапе создаются различные графики и диаграммы, чтобы наглядно представить результаты анализа заказчику. Это делает информацию более понятной и помогает определить дальнейшие действия на её основе.

Интерпретация и выводы. На этом этапе формулируются выводы на основе проведённого анализа. Например, если анализ данных показал, что популярность набирают фильмы жанров триллер и хоррор, то можно сделать вывод, что самое время выпустить новый фильм ужасов.

В основном, выводы делают не аналитики, а заказчики, которые получили результаты анализа. Мы предоставляем и визуализируем данные, но решения в любом случае принимают они. Хотя мы можем предложить свои рекомендации, окончательное решение остаётся за заказчиком.

Сферы применения анализа данных

Анализ данных широко применяется для прогнозирования трендов и событий. С его помощью можно предсказать спрос на продукцию и финансовые показатели.

Анализ данных также используется для принятия решений. Например, если компания планирует ввести новый продукт, анализ данных поможет оценить его потенциальную популярность и определить, стоит ли его выпускать, опираясь в том числе на данные других компаний.

Кроме того, анализ данных применяется для оптимизации процессов, позволяя выявлять неэффективные процессы, улучшать их или исключать те операции, которые показали свою неэффективность.

В научных исследованиях анализ данных помогает проверять гипотезы и оценивать различные показатели. Он также используется в маркетинге и продажах для более точного понимания рынка и потребностей клиентов.

В области анализа данных, особенно с развитием искусственного интеллекта, представлено множество вариантов для трудоустройства. Эта сфера очень популярна, и специалисты по анализу данных востребованы в различных секторах и отраслях.

Сфера анализа данных тесно связана с другими областями, работающими с искусственным интеллектом. Это дата-сайентисты, дата-аналитики, дата-инженеры и т.д. Да, это разные направления, но все они имеют общие точки соприкосновения. После начального опыта в анализе данных можно углубиться в более сложные области и попробовать свои силы в более сложных профессиях. Например, дата-сайентистам требуются глубокие математические знания для работы с формулами и проведения вычислений. Если анализ данных тебя увлечёт, ты можешь в дальнейшем углубиться в детали этих профессий.

2. Работа с датасетами через функции Pandas


Документ для работы:


12.99 КБ	
World-happiness-report-2024.csv
Переходим к практике работы в Python, а точнее, к взаимодействию с библиотекой Pandas.

✅Библиотека Pandas — одна из библиотек Python, предназначенная для анализа данных и манипуляций с данными. Она предоставляет множество полезных функций для работы с таблицами.

Чтобы работать с данными, сначала их надо собрать. В прошлом уроке мы создали таблицу (датасет) с вакансиями, но в ней нет массива данных, который стоит подвергать анализу. Поэтому мы возьмём другую информацию. Для этого обратимся на сайт Kaggle.

✅Kaggle.com — это онлайн-платформа, предоставляющая пользователям доступ к большим наборам данных. Kaggle в том числе предоставляет обширную библиотеку датасетов, которые можно использовать для обучения, исследования и разработки моделей.

✅Датасет — это структурированный набор данных, представленный в табличной форме. Датасеты используются для анализа, обучения моделей машинного обучения и других задач.

1. Заходим на сайте kaggle.com и на панели слева заходим в раздел Datasets. Мы выбираем из библиотеки датасет World Happiness Report-2024 — “Отчёт об уровне счастья в разных странах мира”. Чтобы скачать датасет, нужно нажать кнопку Download в правом верхнем углу сайта.

Файл с этого сайта скачивается в zip-архиве. Чтобы открыть архив, на системе Mac достаточно просто открыть его щелчком мыши; на ОС Windows нужно извлечь содержимое архива в папку.

2. Откроем (извлечём) архив и рассмотрим его содержимое. В нём находится два файла формата csv — датасеты, с которыми можно работать. При анализе также можно использовать и файлы формата Excel.



3. Перенесём этот датасет в свой проект. Это можно сделать, просто перетащив файл мышью из исходной папки в открытый проект на PyCharm, после чего в открывшейся плашке нажать кнопку Refactor и нажать ОК, если приложение PyCharm запросит доступ к файлам на рабочем столе.



4. Для предстоящего анализа установим библиотеку Pandas. Для этого заходим в Python Packages (вторая иконка сверху на панели инструментов PyCharm), в строке поиска вводим pandas и нажимаем Install package.


5. Загрузка завершена, можно начинать работать с Pandas. Импортируем Pandas, при этом для удобства дальнейшей работы переименуем его в pd, для этого прописываем:

import pandas as pd

Функции и команды библиотеки Pandas

Перед тем, как приступить к анализу данных, разберёмся, какие для этого существуют функции и команды. Существует две возможные структуры, с которыми мы будем работать: Series и Dataframe.

Структура Series

✅Series в Pandas — это одномерная структура данных, похожая на массив или список, которая может содержать данные любого типа (числа, строки и т.д.). Каждое значение в Series имеет уникальный индекс, который используется для доступа к элементам. Индекс может быть числовым или строковым, что позволяет легко выбирать и манипулировать данными.

Чтобы наглядно разобраться, что такое Series, создадим список из чисел от 1 до 5, для этого прописываем: data = [1, 2, 3, 4, 5]

Сохраним этот список в переменную series, при этом используем функцию из библиотеки Pandas, которая также называется Series. Прописываем: series = pd.Series(data)

Функция Series преобразует созданный числовой список в массив с индексом. Это похоже на своеобразный столбец в таблице.

Чтобы посмотреть, что получилось, прописываем: print(series)

В первом столбце мы видим индексы, как в обычном списке, от 0 и далее, а во втором столбце — ряды (ячейки) таблицы.


Структура Dataframe

Рассмотрим вторую структуру данных — датафрейм. Это одна из основных структур, используемых в работе.

✅Датафрейм в Pandas — это двумерная структура данных, похожая на таблицу, где данные организованы в строках и столбцах. При этом в каждой строке хранится информация об одном объекте, а в каждом столбце — одно из свойств, то есть, характеристики объекта.

1. Создадим датафрейм. Для этого нужно создать словарь с названиями столбцов и значениями, которые будут содержаться в этих столбцах.
    
  data = {
    'Name':'',
    'Age':'',
    'City':''
    }
    
2. Мы задали названия столбцов. Однако в таблице может быть несколько рядов — например, несколько имён. Добавляем в столбец Name имена в кавычках и через запятую; в столбец Age прописываем возраст в том же порядке, что и имена. При этом кавычки в этом столбце можно не использовать. Таким же образом прописываем города в столбец City (в кавычках). В итоге получается:
    
    data = {
    'Name': ['Alice', 'Bob', 'Roma', 'Anna'],
    'Age': [23, 45, 17, 24],
    'City': ['New York', 'LA', 'Chicago', 'Moscow']
    }



3. Преобразуем созданный словарь data в датафрейм, то есть, в таблицу. Для этого используем функцию df (сокращение от dataframe). Прописываем:

df = pd.DataFrame(data)



4. Посмотрим, что получилось, для этого пишем: print(df) и, щёлкнув правой кнопкой мыши, выбираем в открывшемся списке Run, тем самым запуская код.


PyCharm в целом не очень удобен для работы с анализом данных, поскольку результат здесь не так нагляден, как, например, в Jupyter Notebook. Также для работы с данными можно использовать Google Colab, однако на территории РФ эта программа недоступна.

Основные функции Pandas

Перейдём к работе с готовым датасетом, который мы скачали с сайта Kaggle. Для этого используем функцию, считывающую информацию из csv-файлов. В круглых скобках указываем название файла (его можно скопировать, щёлкнув по файлу правой кнопкой мыши и выбрав в списке Copy.

df = pd.read_csv('World-happiness-report-2024.csv')

Если датасет находится в формате Excel, вместо csv необходимо прописать read_excel.
С помощью функции head вывести пять первых строк, хранящихся в csv-файле. Можно вывести и большее/меньшее количество строк — для этого надо указать точное их количество в круглых скобках после прописанной функции. Для этого прописываем: print(df.head()) и запускаем (Run).

В окне вывода мы видим первые пять строк датасета. При этом столбец с многоточиями указывает на то, что между колонками пропущено ещё какое-то количество столбцов — это происходит потому, что вся таблица не умещается в окне вывода. Однако мы можем видеть информацию о том, что в датасете имеется 12 колонок.


3. Если нужно просмотреть последние строки датасета, вместо функции head используем функцию tail, для этого пишем: print(df.tail()) Таким образом, мы можем узнать, что в этом датасете всего 142 строки.


4. Чтобы посмотреть базовую информацию о датафрейме, используем функцию info. Прописываем: print(df.info())

Здесь выводится информация о том, сколько всего данных содержится в датасете, названия всех столбцов и какое количество столбцов содержит в себе информацию, а также тип хранящихся данных, сколько памяти тратится.


5. Информацию о датафрейме также можно почерпнуть, прописав функцию describe. Эта функция позволяет получить статистические данные: найти минимальные, максимальные и средние значения, количество и т.д. Для этого прописываем: print(df.describe())


6. Также с помощью функций можно выводить конкретную информацию, например, сведения из определённого столбца. Для этого используем название датафрейма (df) и название интересующего столбца. Прописываем: print(df['Country name'])



7. Запускаем код и в окне вывода получаем результат. При этом, опять же, PyCharm не покажет все имеющиеся строки — он выводит первые пять и последние пять строк, заменив остальные строки многоточием.


8. Чтобы вывести одновременно несколько столбцов, нужно названия столбцов заключить в двойные квадратные скобки и указать названия столбцов через запятую. Прописываем: print(df[['Country name', 'Regional indicator']]) и запускаем код.


9. Также можно вывести не весь столбец, а определённую строку; для этого надо использовать функцию loc и указать индекс строки. Прописываем: print(df.loc[56])

В результате в окне вывода мы видим всю информацию, которая есть в строке 56 по всем колонкам.


10.  Если нас интересует не вся имеющаяся в строке информация, нужно выводить строку не по индексу, а с указанием номера строки и названием интересующего нас столбца в кавычках. Название столбца можно скопировать из предыдущего окна вывода. Прописываем: print(df.loc[56, 'Healthy life expectancy']).

В результате мы получаем только один показатель по конкретной строке:


11. Можно вывести информацию, отвечающую определённому условию. Например, нам необходимо найти в столбце Healthy life expectancy все показатели выше 0.7. Для этого необходимо написать: print(df.[df['Healthy life expectancy'] > 0.7])

Таким образом, в этой строке мы указали, что из датафрейма необходимо извлечь информацию из конкретного столбца, которая при этом подчиняется конкретному условию.

В результате мы видим, что в анализируемом датафрейме всего 18 рядов отвечает этому условию.


В этой части урока мы рассмотрели инструменты и функции, позволяющие изучить информацию, содержащуюся в датафрейме. Такие действия необходимы для того, чтобы перед началом анализа понять, какая именно информация содержится в датафрейме, обработана ли она и как с ней можно и нужно работать. Изучив этот датафрейм, мы убедились, что информация в нём обработана и структурирована, тип данных соответствует требованиям, и в целом датасет хорошо оформлен и готов к анализу. Эти знания важны для того, чтобы аналитик понимал, какими данными он располагает, и имел общее представление о материале, с которым будет работать.

3. Функции редактирования и анализа датафрейма


Документы для работы:


7.41 КБ	
hh.csv

540 Б	
animal.csv
Продолжаем работать в Pandas. Если раньше мы просто рассматривали данные, сейчас мы попробуем взаимодействовать с таблицей данных — например, добавим или удалим столбцы. Работать мы будем с таблицей hh.csv из прошлого урока.

Добавление и удаление столбцов и строк с помощью функций Pandas

Для начала необходимо считать таблицу, для этого прописываем команду read, а в круглых скобках указываем название файла, содержащего нужную таблицу:

df = pd.read_csv('hh.csv')

Добавим к таблице столбец. Для этого после переменной df нужно прописать название столбца, который мы будем создавать. Мы создадим столбец с названием Test и положим в него столько значений, сколько у нас уже есть:

sd['Test'] = [new for new in range(29)]

Цикл for здесь будет перебирать список, который создаётся с помощью команды range из 29 чисел (от 0 до 28) и будет подставлять их в список, где они будут сохраняться.

Выведем результат, для этого прописываем:

print(df)

Видим, что к таблице добавился столбец Test:


4. Рассмотрим, как удалять столбцы и строки в датасете. Для этого используется функция drop. Если надо удалить столбец, указываем в круглых скобках название этого столбца. Также после названия столбца мы используем параметр axis со значением либо 0 (в случае удаления строк), либо 1 (в случае удаления столбцов).

Следующий параметр, который мы используем — это inplace. Он нужен, чтобы показывать, что изменения, которые мы вносим, должны быть внесены в исходный датафрейм — в этом случае для этого параметра прописывается значение True. В случае, если значение этого параметра False, оригинальный датафрейм останется неизменным. Учитывая всё вышесказанное, прописываем:

df.drop('Test', axis=1, inplace=True)

Чтобы увидеть результат, пишем print(df)

Видим, что столбец Test удалён, и в качестве последнего столбца в окне вывода теперь показывается столбец "Ссылка на вакансию".



5. Удалим конкретную строку. Как мы уже узнаем, для этого параметр axis должен быть равен 0. Вместо названия столбца мы указываем индекс строки. Прописываем:

df.drop(28, axis=0, inplace=True)

print(df)

Теперь последняя, 28-я строка, удалена из датафрейма.

Все внесённые изменения пока не сохранены. Как сохранить их, мы узнаем чуть позже.

Редактирование и удаление данных

Аналитики порой сталкиваются с ситуацией, когда в датасете недостаёт информации, что повлияет на результат либо вообще помешает анализу, поскольку, когда в ячейке нет информации, она имеет тип данных NaN и с ней надо работать по-другому. Такие значения нужно заменять на значения, которые не помешают проводить анализ.

Рассмотрим подобные случаи на примере небольшого датасета. В этом датасете есть столбец Популярность, но значение популярности задано не для всех животных — у кошки и лошади эти значения пропущены (NaN).


1. После добавления файла с этим датасетом в проект его необходимо считать. Для этого прописываем: df.read_csv(animal.csv). Далее пишем: print(df) и запускаем код через Run. Как мы уже говорили, в строках, относящихся к кошке и лошади, в столбце Популярность указаны значения NaN.


Значения NaN могут помешать дальнейшему анализу — их надо либо заменить, либо удалить. Удаление данных — не оптимальный вариант, но в рамках этого урока мы рассмотрим как удаление данных, так и их замену.



2. Чтобы заменить значение NaN на какое-либо другое, нужно использовать команду fillna, заполняющую все пропуски. Прописываем эту команду и сразу за ней print, чтобы посмотреть на результат.

df.fillna(value:0, inplace=True) print(df)

Теперь в строках, относящихся к кошке и лошади, значение Популярность поменялось на 0.0.


3. Если мы не хотим заменять значения, можно их удалить, хотя это всё-таки делать не рекомендуется. Для этого используем команду dropna:

df.dropna(inplace=True)

В этом случае все строки Кошка и Лошадь будут полностью удалены из датасета.



4. Данные можно группировать по конкретным характеристикам, чтобы, например, выводить средние значения. В рассматриваемом датасете мы можем сгруппировать данные, например, по виду корма — растительный или мясной. Представим, что нам надо узнать среднюю продолжительность жизни животных в зависимости от того, чем они питаются. Для создания группы мы используем переменную group, для подсчёта среднего значения — mean. Прописываем:

group = df.groupby('Пища')['Средняя продолжительность жизни']mean()

Функция groupby применяется не только в библиотеке Pandas для анализа данных в Python, но и в языке структурированных запросов (SQL). Если ты уже изучал учебник, приложенный к урокам, возможно, ты уже знаком с этой функцией.
5. Посмотрим на результат, прописываем print(group) и запускаем код (Run).


Заметим, что в этом датасете собраны специфические данные при небольшой выборке с явным перевесом категории животных, питающихся растительным кормом, поэтому результаты такого анализа могут выглядеть нерепрезентативно.



6. Чтобы сохранить изменения в таблицах (удаление и добавление столбцов, строк и т.д.), необходимо использовать функцию to_csv, а в круглых скобках указать место сохранения. Для сохранения данных прописываем:

df.to_csv('output.csv', index=False)

Сохранение изменений в файле

В качестве примера возьмём созданный в предыдущей части урока словарь с указанием имён, возраста и городов, и сохраним его.

Скопируем из предыдущей части урока:

data = { 'Name': ['Alice', 'Bob', 'Roma', 'Anna'], 'Age': [23, 45, 17, 24], 'City': ['New York', 'LA', 'Chicago', 'Moscow'] }

Чтобы создать датафрейм, прописываем: df = pd.DataFrame(data)

Теперь сохраним его в отдельный файл: df.to_csv('output.csv', index=False)

Запускаем код и получаем отдельный файл output.csv с сохранённой в нём информацией.


Если в дальнейшей работе с этим датасетом в него будут внесены изменения, их нужно будет снова сохранить аналогичным образом.

4. Результаты урока


Сегодня мы узнали, что такое анализ данных и для чего он нужен, а также изучили основы использования библиотеки Pandas. Мы поработали с различными датасетами и рассмотрели множество различных функций.

В качестве домашнего задания необходимо выполнить две небольшие задачи. Нужно скачать и изучить датасет с сайта Kaggle, после чего попробовать выводить информацию — подробнее об этом описано в разделе Домашнее задание.

Во второй задаче необходимо определить среднюю зарплату. К уроку прикреплён файл dz.csv, который нужно проанализировать, изучить его структуру и попробовать найти среднее значение заработной платы, сгруппировав данные по городам.

А на следующем уроке мы продолжим изучение библиотеки Pandas, углубившись в анализ данных.

Дополнительные материалы

Где брать датасеты: https://www.kaggle.com/datasets

Можно попробовать работать в jupyter notebook: https://jupyter.org/try

Статьи для погружения в анализ данных с библиотекой pandas:

https://khashtamov.com/ru/pandas-introduction/

https://education.yandex.ru/handbook/python/article/modul-pandas

https://habr.com/ru/companies/ruvds/articles/494720/

https://habr.com/ru/companies/otus/articles/727222/